{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "G = nx.DiGraph()\n",
    "edges = [(0, 1, 2.5), (1, 2, 1.5), (2, 0, 3.0), (2, 3, 2.0), (3, 4, 1.0)]\n",
    "G.add_weighted_edges_from(edges)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_from_network(trade_data):\n",
    "    \"\"\"\n",
    "    Analyzes trade network using Infomap community detection.\n",
    "    \n",
    "    Parameters:\n",
    "    trade_data (pd.DataFrame): DataFrame with columns 'reporterDesc', 'partnerDesc', and 'primaryValue'\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (node_communities, node_mapping)\n",
    "    \"\"\"\n",
    "    # Create a graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges to the graph, excluding 'World'\n",
    "    for index, row in trade_data.iterrows():\n",
    "        source = row['reporterDesc']\n",
    "        dest = row['partnerDesc']\n",
    "        value = row['primaryValue']\n",
    "        if source != 'World' and dest != 'World':\n",
    "            if G.has_edge(source, dest):\n",
    "                G[source][dest]['weight'] += value\n",
    "            else:\n",
    "                G.add_edge(source, dest, weight=value)\n",
    "    # # Normalize the weights of the edges\n",
    "    # for u in G.nodes():\n",
    "    #     total_weight = sum(data['weight'] for _, _, data in G.edges(u, data=True))\n",
    "    #     for v in G.successors(u):\n",
    "    #         G[u][v]['weight'] /= total_weight\n",
    "    # Create a mapping between string names and numeric IDs\n",
    "    nodes = list(G.nodes())\n",
    "    node_to_id = {node: idx for idx, node in enumerate(nodes, start=1)}\n",
    "    id_to_node = {idx: node for node, idx in node_to_id.items()}\n",
    "\n",
    "    # Print network statistics\n",
    "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "    \n",
    "    # # Print the edges with weights\n",
    "    # print(\"\\nEdges with weights:\")\n",
    "    # for u, v, data in G.edges(data=True):\n",
    "    #     print(f\"({u}, {v}, {data})\")\n",
    "\n",
    "    \n",
    "\n",
    "    return G, node_to_id,id_to_node\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Read the trade data\n",
    "trade_data_2024 = pd.read_excel('TradeData.xlsx')\n",
    "\n",
    "# Analyze the network\n",
    "G, node_to_id, id_to_node = get_graph_from_network(trade_data_2024)\n",
    "centrality = nx.betweenness_centrality(G)\n",
    "centrality_keys = list(reversed(sorted(centrality.keys(),key = lambda x: centrality[x])))\n",
    "print([(key,centrality[key]) for key in centrality_keys[:5]])\n",
    "print(len(nx.community.louvain_communities(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_directed_graph(edges):\n",
    "    G = nx.DiGraph()  # Create a directed graph\n",
    "    G.add_weighted_edges_from(edges)  # Add edges with weights\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betweenness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_betweenness_centrality(G):\n",
    "    # Calculate betweenness centrality with weights\n",
    "    betweenness_centrality = nx.betweenness_centrality(G, weight='weight', normalized=True)\n",
    "    return betweenness_centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out degree Centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-degree centrality for each node:\n",
      "Node 0: 0.5000\n",
      "Node 1: 1.0000\n",
      "Node 2: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def calculate_out_degree_centrality(G):\n",
    "    \"\"\"\n",
    "    Calculates the out-degree centrality for each node in a directed weighted graph,\n",
    "    ignoring the weights on the edges.\n",
    "    :param G: A directed weighted graph (NetworkX DiGraph).\n",
    "    :return: Dictionary with nodes as keys and their out-degree centrality as values.\n",
    "    \"\"\"\n",
    "    N = len(G.nodes)  # Total number of nodes\n",
    "    centrality = {}\n",
    "\n",
    "    for node in G.nodes:\n",
    "        # Calculate the out-degree (number of outgoing edges, without considering weights)\n",
    "        out_degree = G.out_degree(node)\n",
    "        \n",
    "        # Calculate out-degree centrality\n",
    "        centrality[node] = out_degree / (N - 1) if N > 1 else 0  # Avoid division by zero for single-node graph\n",
    "\n",
    "    return centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radiality centrality :- https://search.r-project.org/CRAN/refmans/centiserve/html/radiality.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eccentricity and Closeness and Radiality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def calculate_metrics(G):\n",
    "    \"\"\"\n",
    "    Calculates radiality, eccentricity, and closeness centrality for each node in a directed weighted graph.\n",
    "    :param G: A directed weighted graph (NetworkX DiGraph).\n",
    "    :return: Three dictionaries with nodes as keys for radiality, eccentricity, and closeness centrality.\n",
    "    \"\"\"\n",
    "    N = len(G.nodes)  # Total number of nodes\n",
    "    radiality = {}\n",
    "    eccentricity = {}\n",
    "    closeness_centrality = {}\n",
    "\n",
    "    # Step 1: Calculate all pairs shortest paths in the directed weighted graph using Dijkstra\n",
    "    shortest_paths = dict(nx.all_pairs_dijkstra_path_length(G, weight=\"weight\"))\n",
    "    \n",
    "    # Step 2: Calculate the graph diameter as the longest shortest path distance\n",
    "    # Note: This considers only reachable pairs to avoid infinity issues\n",
    "    diameter = max(\n",
    "        max(lengths.values()) for lengths in shortest_paths.values() if lengths\n",
    "    )\n",
    "\n",
    "    for v in G.nodes:\n",
    "        # Initialize variables for radiality, eccentricity, and closeness centrality for node v\n",
    "        sum_adjusted_paths = 0\n",
    "        total_distance = 0\n",
    "        max_distance = 0\n",
    "\n",
    "        # Step 3: Calculate metrics based on shortest path distances\n",
    "        for u in G.nodes:\n",
    "            if u != v:\n",
    "                # Get the shortest path distance from v to u\n",
    "                distance_vu = shortest_paths[v].get(u, float('inf'))\n",
    "\n",
    "                # Radiality: Sum (G + 1 - distance(v, u)) for each reachable node u\n",
    "                if distance_vu < float('inf'):\n",
    "                    adjusted_distance = (diameter + 1 - distance_vu)\n",
    "                    sum_adjusted_paths += adjusted_distance\n",
    "                    total_distance += distance_vu\n",
    "                    max_distance = max(max_distance, distance_vu)\n",
    "\n",
    "        # Radiality for node v\n",
    "        radiality[v] = sum_adjusted_paths / (N - 1) if N > 1 else 0\n",
    "\n",
    "        # Eccentricity for node v (maximum distance to any reachable node)\n",
    "        eccentricity[v] = max_distance if max_distance > 0 else float('inf')\n",
    "\n",
    "        # Closeness centrality for node v (inverse of average shortest path distance)\n",
    "        closeness_centrality[v] = 1 / total_distance if total_distance > 0 else 0\n",
    "\n",
    "    return radiality, eccentricity, closeness_centrality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to implement the Trade_weight matrix, find the bottleneck node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the bottleneck is graph before news comes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics on Trade-Weighted Graph:\n",
      "Top Radialities: [(2, 4.625), (1, 4.5), (0, 3.125), (3, 1.75), (4, 0.0)]\n",
      "Top Eccentricities: [(4, inf), (0, 7.0), (2, 5.5), (1, 4.5), (3, 1.0)]\n",
      "Top Closeness Centralities: [(3, 1.0), (2, 0.07407407407407407), (1, 0.07142857142857142), (0, 0.05128205128205128), (4, 0)]\n",
      "Top Betweenness Centralities: [(2, 0.41666666666666663), (1, 0.25), (3, 0.25), (0, 0.08333333333333333), (4, 0.0)]\n",
      "\n",
      "Metrics on Loss-Weighted Graph:\n",
      "Top Radialities: [(2, 3.5), (1, 3.0), (0, 2.5), (3, 1.0), (4, 0.0)]\n",
      "Top Eccentricities: [(4, inf), (0, 4.0), (1, 3.0), (2, 2.0), (3, 1.0)]\n",
      "Top Closeness Centralities: [(3, 1.0), (2, 0.16666666666666666), (1, 0.125), (0, 0.1), (4, 0)]\n",
      "Top Betweenness Centralities: [(2, 0.41666666666666663), (1, 0.25), (3, 0.25), (0, 0.08333333333333333), (4, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_loss_weight_adj_matrix(G):\n",
    "    L = nx.to_numpy_array(G, weight=\"weight\")\n",
    "    \n",
    "    # Transpose the matrix to make each column represent imports for normalization\n",
    "    L_T = L.T\n",
    "    \n",
    "    # Normalize each row of the transposed matrix to get import-based percentages\n",
    "    for i in range(L_T.shape[0]):\n",
    "        col_sum = np.sum(L_T[i])\n",
    "        if col_sum > 0:  # Avoid division by zero for nodes with no imports\n",
    "            L_T[i] = L_T[i] / col_sum\n",
    "    \n",
    "    # Transpose back to the original orientation\n",
    "    L_normalized = L_T.T\n",
    "    return L_normalized\n",
    "\n",
    "\n",
    "# Metrics on trade-weighted graph\n",
    "radiality, eccentricity, closeness_centrality = calculate_metrics(G)\n",
    "betweenness_centrality = calculate_betweenness_centrality(G)\n",
    "\n",
    "# Metrics on loss-weighted graph\n",
    "# Create a directed weighted graph using the loss-weighted adjacency matrix\n",
    "L = create_loss_weight_adj_matrix(G)\n",
    "G_loss = nx.from_numpy_array(L, create_using=nx.DiGraph)\n",
    "radiality_loss, eccentricity_loss, closeness_centrality_loss = calculate_metrics(G_loss)\n",
    "betweenness_centrality_loss = calculate_betweenness_centrality(G_loss)\n",
    "\n",
    "# Function to get top 5 nodes based on metric values\n",
    "def get_top_5(metric_dict):\n",
    "    return sorted(metric_dict.items(), key=lambda item: item[1], reverse=True)[:5]\n",
    "\n",
    "# Metrics on trade-weighted graph\n",
    "radiality, eccentricity, closeness_centrality = calculate_metrics(G)\n",
    "betweenness_centrality = calculate_betweenness_centrality(G)\n",
    "\n",
    "print(\"\\nMetrics on Trade-Weighted Graph:\")\n",
    "print(\"Top Radialities:\", get_top_5(radiality))\n",
    "print(\"Top Eccentricities:\", get_top_5(eccentricity))\n",
    "print(\"Top Closeness Centralities:\", get_top_5(closeness_centrality))\n",
    "print(\"Top Betweenness Centralities:\", get_top_5(betweenness_centrality))\n",
    "\n",
    "# Metrics on loss-weighted graph\n",
    "# Create a directed weighted graph using the loss-weighted adjacency matrix\n",
    "L = create_loss_weight_adj_matrix(G)\n",
    "G_loss = nx.from_numpy_array(L, create_using=nx.DiGraph)\n",
    "radiality_loss, eccentricity_loss, closeness_centrality_loss = calculate_metrics(G_loss)\n",
    "betweenness_centrality_loss = calculate_betweenness_centrality(G_loss)\n",
    "\n",
    "print(\"\\nMetrics on Loss-Weighted Graph:\")\n",
    "print(\"Top Radialities:\", get_top_5(radiality_loss))\n",
    "print(\"Top Eccentricities:\", get_top_5(eccentricity_loss))\n",
    "print(\"Top Closeness Centralities:\", get_top_5(closeness_centrality_loss))\n",
    "print(\"Top Betweenness Centralities:\", get_top_5(betweenness_centrality_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communities: [{0, 1, 2}, {3, 4}]\n"
     ]
    }
   ],
   "source": [
    "# Given a digraph G with edge weights, run the louvain community detection algorithm\n",
    "import community as community_louvain\n",
    "communities = nx.community.louvain_communities(G, weight='weight')\n",
    "print(\"Communities:\", communities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the news, if news says few nodes are going to get disrupted, check whether those nodes were bottlenecks in the graph before. \n",
    "### If yes, then Issue A Warning, if not we do not care"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once communites are made, create subgraphs out of these communities, and again run the matrics on these subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def create_community_subgraphs(G, communities):\n",
    "    \"\"\"\n",
    "    Creates subgraphs for each community, keeping only the edges within the community.\n",
    "    :param G: Directed weighted graph (NetworkX DiGraph).\n",
    "    :param communities: List of sets, where each set represents a community of nodes.\n",
    "    :return: List of subgraphs, one for each community.\n",
    "    \"\"\"\n",
    "    subgraphs = []\n",
    "    for community in communities:\n",
    "        # Create subgraph for the current community\n",
    "        subgraph = G.subgraph(community).copy()  # Use copy to create an independent subgraph\n",
    "        subgraphs.append(subgraph)\n",
    "    return subgraphs\n",
    "\n",
    "# Communities detected in the format: [{0, 1, 2}, {3, 4}]\n",
    "communities = nx.community.louvain_communities(G, weight='weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Metrics for Community Subgraph 1 (Nodes: [0, 1, 2]) ---\n",
      "\n",
      "Trade Matrix Metrics (Top 5 Nodes):\n",
      "Top Radialities: [(1, 3.5), (0, 3.25), (2, 2.25)]\n",
      "Top Eccentricities: [(2, 5.5), (1, 4.5), (0, 4.0)]\n",
      "Top Closeness Centralities: [(1, 0.16666666666666666), (0, 0.15384615384615385), (2, 0.11764705882352941)]\n",
      "Top Betweenness Centralities: [(0, 0.5), (1, 0.5), (2, 0.5)]\n",
      "\n",
      "Loss Matrix Metrics (Top 5 Nodes):\n",
      "Top Radialities: [(0, 1.5), (1, 1.5), (2, 1.5)]\n",
      "Top Eccentricities: [(0, 2.0), (1, 2.0), (2, 2.0)]\n",
      "Top Closeness Centralities: [(0, 0.3333333333333333), (1, 0.3333333333333333), (2, 0.3333333333333333)]\n",
      "Top Betweenness Centralities: [(0, 0.5), (1, 0.5), (2, 0.5)]\n",
      "\n",
      "--- Metrics for Community Subgraph 2 (Nodes: [3, 4]) ---\n",
      "\n",
      "Trade Matrix Metrics (Top 5 Nodes):\n",
      "Top Radialities: [(3, 1.0), (4, 0.0)]\n",
      "Top Eccentricities: [(4, inf), (3, 1.0)]\n",
      "Top Closeness Centralities: [(3, 1.0), (4, 0)]\n",
      "Top Betweenness Centralities: [(3, 0.0), (4, 0.0)]\n",
      "\n",
      "Loss Matrix Metrics (Top 5 Nodes):\n",
      "Top Radialities: [(0, 1.0), (1, 0.0)]\n",
      "Top Eccentricities: [(1, inf), (0, 1.0)]\n",
      "Top Closeness Centralities: [(0, 1.0), (1, 0)]\n",
      "Top Betweenness Centralities: [(0, 0.0), (1, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "community_subgraphs = create_community_subgraphs(G, communities)\n",
    "\n",
    "for i, subgraph in enumerate(community_subgraphs):\n",
    "    print(f\"\\n--- Metrics for Community Subgraph {i+1} (Nodes: {list(subgraph.nodes)}) ---\")\n",
    "\n",
    "    # Find metrics for each community subgraph\n",
    "    radiality, eccentricity, closeness_centrality = calculate_metrics(subgraph)\n",
    "    betweenness_centrality = calculate_betweenness_centrality(subgraph)\n",
    "    \n",
    "    # Function to get top 5 nodes based on metric values\n",
    "    def get_top_5(metric_dict):\n",
    "        return sorted(metric_dict.items(), key=lambda item: item[1], reverse=True)[:5]\n",
    "    \n",
    "    print(\"\\nTrade Matrix Metrics (Top 5 Nodes):\")\n",
    "    print(\"Top Radialities:\", get_top_5(radiality))\n",
    "    print(\"Top Eccentricities:\", get_top_5(eccentricity))\n",
    "    print(\"Top Closeness Centralities:\", get_top_5(closeness_centrality))\n",
    "    print(\"Top Betweenness Centralities:\", get_top_5(betweenness_centrality))\n",
    "\n",
    "    # Find metrics on the loss-weighted graph for each community subgraph\n",
    "    L = create_loss_weight_adj_matrix(subgraph)\n",
    "    G_loss = nx.from_numpy_array(L, create_using=nx.DiGraph)\n",
    "    \n",
    "    radiality_loss, eccentricity_loss, closeness_centrality_loss = calculate_metrics(G_loss)\n",
    "    betweenness_centrality_loss = calculate_betweenness_centrality(G_loss)\n",
    "    \n",
    "    print(\"\\nLoss Matrix Metrics (Top 5 Nodes):\")\n",
    "    print(\"Top Radialities:\", get_top_5(radiality_loss))\n",
    "    print(\"Top Eccentricities:\", get_top_5(eccentricity_loss))\n",
    "    print(\"Top Closeness Centralities:\", get_top_5(closeness_centrality_loss))\n",
    "    print(\"Top Betweenness Centralities:\", get_top_5(betweenness_centrality_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
